# 3D ê°ì²´ íƒì§€ ëª¨ë¸ ìµœì í™”ë¥¼ ìœ„í•œ í™•ë¥ ì  ê²½ì‚¬ MCMC ê¸°ë²• ì ìš©ì— ëŒ€í•œ íƒìƒ‰ì  ì—°êµ¬

**Exploratory Study on Applying Stochastic Gradient MCMC for 3D Object Detection Model Optimization**

[![Paper](https://img.shields.io/badge/Paper-PDF-red)](paper/research_paper.pdf)
[![KITTI](https://img.shields.io/badge/Dataset-KITTI-blue)](http://www.cvlibs.net/datasets/kitti/)
[![OpenPCDet](https://img.shields.io/badge/Framework-OpenPCDet-green)](https://github.com/open-mmlab/OpenPCDet)

## ğŸ“‹ ê°œìš” (Overview)

ë³¸ ì—°êµ¬ëŠ” ììœ¨ì£¼í–‰ì„ ìœ„í•œ 3D ê°ì²´ ê²€ì¶œì—ì„œ **GCIoU (Generalized Complete IoU) ì†ì‹¤**ì„ ì‚¬ìš©í•˜ëŠ” ëª¨ë¸ì— **Stochastic Gradient MCMC (SGMCMC)** ê¸°ë²•ì„ ì ìš©í•˜ì—¬, ê¸°ì¡´ Adam optimizerì™€ì˜ ì„±ëŠ¥ì„ ë¹„êµ ë¶„ì„í•©ë‹ˆë‹¤.

### ì£¼ìš” íŠ¹ì§•
- **Dataset**: KITTI 3D Object Detection Benchmark
- **Framework**: OpenPCDet with PointPillars architecture
- **Loss Function**: GCIoU (Gradient-Corrected IoU)
- **Optimizers Compared**: Adam, SGD, SGHMC, SGLD, SGNHT

## ğŸ¯ ì—°êµ¬ ëª©ì 

3D ê°ì²´ íƒì§€ì—ì„œ IoU ê¸°ë°˜ íšŒê·€ ì†ì‹¤ì˜ gradient vanishing ë¬¸ì œë¥¼ í•´ê²°í•œ GCIoU ì†ì‹¤ì´ Adam optimizerì™€ í•¨ê»˜ ì œì•ˆë˜ì—ˆìŠµë‹ˆë‹¤. ë³¸ ì—°êµ¬ëŠ” ë™ì¼í•œ ì—ë„ˆì§€ í•¨ìˆ˜ë¥¼ ìœ ì§€í•˜ë©´ì„œ SGMCMC ê¸°ë²•ë“¤ì„ ì ìš©í•˜ì—¬ **Bayesian sampling ê¸°ë°˜ ìµœì í™”ì˜ ì‹¤ìš©ì„±**ì„ ì²´ê³„ì ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.

## ğŸ”¬ ì‹¤í—˜ ì„¤ì •

### í•˜ì´í¼íŒŒë¼ë¯¸í„°
- **Training Epochs**: 80
- **Learning Rate Schedule**: OneCycle (ì´ˆê¸° í•™ìŠµë¥ : 0.00035)
- **Batch Size**: 8
- **Weight Decay**: 0.01
- **Architecture**: CT-Stack backbone with PointPillars

### í‰ê°€ ì§€í‘œ
- Bird's Eye View Average Precision (BEV AP)
- Recall@IoU=0.7
- KITTI ë‚œì´ë„ë³„ í‰ê°€ (Easy/Moderate/Hard)

## ğŸ“Š ì£¼ìš” ì‹¤í—˜ ê²°ê³¼

### ì •ëŸ‰ì  ì„±ëŠ¥ ë¹„êµ

| Optimizer | Recall@0.7 | BEV AP Easy (%) | BEV AP Moderate (%) | BEV AP Hard (%) | Final Loss |
|-----------|------------|-----------------|---------------------|-----------------|------------|
| **Adam**  | **0.417**  | **78.58**       | **65.15**           | **63.66**       | 1.149      |
| SGHMC     | 0.341      | 56.31           | 49.59               | 43.12           | **0.813**  |
| SGD       | 0.317      | 55.56           | 49.07               | 42.19           | 0.983      |
| SGLD      | 0.138      | 33.31           | 25.67               | 20.45           | 1.135      |
| SGNHT     | 0.128      | 31.08           | 25.37               | 24.85           | 1.326      |

### í•µì‹¬ ë°œê²¬ì‚¬í•­

1. **Adaptive Learning Rateì˜ ì••ë„ì  ìš°ìœ„**
   - Adamì´ BEV AP Moderate 65.15%ë¡œ ìµœê³  ì„±ëŠ¥ ë‹¬ì„±
   - SGHMC ëŒ€ë¹„ +31.4% ì„±ëŠ¥ í–¥ìƒ
   - ë³µì¡í•œ 3D ê²€ì¶œ ë¬¸ì œì—ì„œ íŒŒë¼ë¯¸í„°ë³„ ì ì‘í˜• í•™ìŠµë¥ ì´ í•„ìˆ˜ì 

2. **Training Lossì™€ Detection ì„±ëŠ¥ì˜ ë¶ˆì¼ì¹˜**
   - SGHMC: ìµœì € loss (0.813) ë‹¬ì„±í–ˆìœ¼ë‚˜ ì„±ëŠ¥ì€ 2ìœ„ (49.59%)
   - Adam: 4ë²ˆì§¸ loss (1.149)ì´ì§€ë§Œ ìµœê³  ì„±ëŠ¥ (65.15%)
   - **Loss minimization â‰  Task performance** ì…ì¦

3. **Momentumì˜ ê²°ì •ì  ì—­í• **
   - SGHMC (49.59%) vs SGLD (25.67%): **147% ì„±ëŠ¥ ì°¨ì´**
   - Momentumì´ stochastic optimizationì˜ ì•ˆì •ì„±ì„ ê·¼ë³¸ì ìœ¼ë¡œ ì¢Œìš°
   - Noise injectionë§Œìœ¼ë¡œëŠ” ë³µì¡í•œ 3D ê²€ì¶œ ë¬¸ì œ í•´ê²° ë¶ˆê°€

4. **Temperature Parameterì˜ ê·¹ì‹¬í•œ ë¯¼ê°ë„**
   - T â‰¥ 1e-3: Gradient explosion ë°œìƒ
   - T = 1e-4: í•™ìŠµ ì™„ì „ ì‹¤íŒ¨ (ì„±ëŠ¥ 0 ìˆ˜ë ´)
   - T = 1e-5: ìœ ì¼í•œ ìœ íš¨ ë²”ìœ„ì´ë‚˜ ì„±ëŠ¥ ì €í•˜
   - Bayesian methodì˜ ì‹¤ìš©ì  ì ìš© ì œí•œ

## ğŸ—ï¸ í”„ë¡œì íŠ¸ êµ¬ì¡°

```
GCIoU-SGMCMC-3D-Detection/
â”œâ”€â”€ README.md
â”œâ”€â”€ paper/
â”‚   â””â”€â”€ research_paper.pdf          # ì—°êµ¬ ë…¼ë¬¸ ì „ë¬¸
â”œâ”€â”€ configs/                         # ì‹¤í—˜ ì„¤ì • íŒŒì¼
â”‚   â”œâ”€â”€ pointpillars_adam.yaml
â”‚   â”œâ”€â”€ pointpillars_sgd.yaml
â”‚   â”œâ”€â”€ pointpillars_sghmc.yaml
â”‚   â”œâ”€â”€ pointpillars_sgld.yaml
â”‚   â””â”€â”€ pointpillars_sgnht.yaml
â”œâ”€â”€ optimizers/                      # SGMCMC ì˜µí‹°ë§ˆì´ì € êµ¬í˜„
â”‚   â”œâ”€â”€ sgld.py
â”‚   â”œâ”€â”€ sghmc.py
â”‚   â””â”€â”€ sgnht.py
â”œâ”€â”€ loss/                           # GCIoU ì†ì‹¤ í•¨ìˆ˜
â”‚   â””â”€â”€ gciou_loss.py
â”œâ”€â”€ experiments/                    # ì‹¤í—˜ ê²°ê³¼ ë° ë¡œê·¸
â”‚   â”œâ”€â”€ results/
â”‚   â””â”€â”€ logs/
â””â”€â”€ requirements.txt
```

## ğŸ”§ ì„¤ì¹˜ ë° ì‚¬ìš©ë²•

### ìš”êµ¬ì‚¬í•­
```bash
# Python 3.8+
# PyTorch 1.10+
# CUDA 11.1+

pip install -r requirements.txt
```

### OpenPCDet ì„¤ì¹˜
```bash
git clone https://github.com/open-mmlab/OpenPCDet.git
cd OpenPCDet
pip install -r requirements.txt
python setup.py develop
```

### KITTI ë°ì´í„°ì…‹ ì¤€ë¹„
1. [KITTI 3D Object Detection](http://www.cvlibs.net/datasets/kitti/eval_object.php?obj_benchmark=3d) ë‹¤ìš´ë¡œë“œ
2. ë°ì´í„° êµ¬ì¡° ì„¤ì •:
```
data/kitti/
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ calib/
â”‚   â”œâ”€â”€ label_2/
â”‚   â”œâ”€â”€ velodyne/
â”‚   â””â”€â”€ image_2/
â””â”€â”€ testing/
    â”œâ”€â”€ calib/
    â”œâ”€â”€ velodyne/
    â””â”€â”€ image_2/
```

### í•™ìŠµ ì‹¤í–‰

#### Adam Optimizer (Baseline)
```bash
python train.py --cfg_file configs/pointpillars_adam.yaml
```

#### SGHMC
```bash
python train.py --cfg_file configs/pointpillars_sghmc.yaml \
    --optimizer sghmc \
    --temperature 1e-5 \
    --friction 0.1
```

#### SGLD
```bash
python train.py --cfg_file configs/pointpillars_sgld.yaml \
    --optimizer sgld \
    --temperature 1e-5
```

#### SGNHT
```bash
python train.py --cfg_file configs/pointpillars_sgnht.yaml \
    --optimizer sgnht \
    --temperature 1e-5 \
    --thermostat_mass 1.0
```

## ğŸ“ˆ SGMCMC êµ¬í˜„ ì„¸ë¶€ì‚¬í•­

### SGLD (Stochastic Gradient Langevin Dynamics)
```python
Î¸_{t+1} = Î¸_t - Î·Â·âˆ‡E(Î¸_t) + âˆš(2Î·T)Â·N(0,I)
```
- ê°€ì¥ ë‹¨ìˆœí•œ SGMCMC ê¸°ë²•
- Random walk íŠ¹ì„±ìœ¼ë¡œ ìƒ˜í”Œ íš¨ìœ¨ ë‚®ìŒ

### SGHMC (Stochastic Gradient Hamiltonian Monte Carlo)
```python
r_{t+1/2} = r_t - (Î·/2)Â·âˆ‡E(Î¸_t) - Î³r_t + N(0, 2Î³Î·T)
Î¸_{t+1} = Î¸_t + Î·Â·r_{t+1/2}
r_{t+1} = r_{t+1/2} - (Î·/2)Â·âˆ‡E(Î¸_{t+1}) - Î³r_{t+1/2}
```
- Momentum ë³€ìˆ˜ ë„ì…ìœ¼ë¡œ ë” ë‚˜ì€ íƒìƒ‰
- Adam ëŒ€ë¹„ 24% ë‚®ì€ ì„±ëŠ¥ì´ë‚˜ SGLDë³´ë‹¤ 147% í–¥ìƒ

### SGNHT (Stochastic Gradient NosÃ©-Hoover Thermostat)
```python
dÎ¸ = M^{-1}rÂ·dt
dr = -âˆ‡E(Î¸)dt - Î¾rÂ·dt
dÎ¾ = (1/Q)(||r||Â² - dT)dt + âˆš(2ÏµT/Q)Â·dW_t
```
- Thermostat ë³€ìˆ˜ë¡œ ì˜¨ë„ ìë™ ì¡°ì ˆ
- ì¶”ê°€ ë³µì¡ì„±ìœ¼ë¡œ ì¸í•´ SGLDë³´ë‹¤ë„ ë‚®ì€ ì„±ëŠ¥

## ğŸ’¡ ì£¼ìš” ê¸°ì—¬

1. **ì‹¤ì¦ì  ë¹„êµ ë¶„ì„**
   - ë³µì¡í•œ 3D vision taskì—ì„œ ë‹¤ì–‘í•œ ìµœì í™” ê¸°ë²•ì˜ ì²´ê³„ì  ë¹„êµ
   - Adaptive learning rate > Stochastic exploration ì…ì¦

2. **Flat Minima ì´ë¡ ì˜ ì‹¤ìš©ì  ê²€ì¦**
   - Training loss minimization â‰  Task performance
   - Task-specific metric ìµœì í™”ì˜ ì¤‘ìš”ì„±

3. **Momentumê³¼ Temperatureì˜ ì—­í•  ê·œëª…**
   - Momentumì˜ ê²°ì •ì  ì¤‘ìš”ì„± (147% ì„±ëŠ¥ ì°¨ì´)
   - Temperature tuningì˜ ê·¹ì‹¬í•œ ì–´ë ¤ì›€

4. **ì‹¤ë¬´ì  ê°€ì´ë“œë¼ì¸ ì œì‹œ**
   - ë³µì¡í•œ vision taskì—ì„œëŠ” Adamê³¼ ê°™ì€ adaptive optimizer ê¶Œì¥
   - Bayesian methodëŠ” uncertainty quantificationì´ í•„ìˆ˜ì ì¸ ê²½ìš°ì—ë§Œ ì œí•œì  ì‚¬ìš©

## ğŸ”® í–¥í›„ ì—°êµ¬ ë°©í–¥

### Hybrid Approaches
- Adamì˜ adaptive mechanism + SGHMCì˜ uncertainty quantification ê²°í•©
- ì¸µë³„ optimizer ì „ëµ (backbone: Adam, detection head: SGHMC)

### Temperature Scheduling
- Curriculum ê¸°ë°˜ noise scheduling
- Adaptive temperature adjustment

### Architecture-Specific Tuning
- ë‹¤ì–‘í•œ 3D ê²€ì¶œ ì•„í‚¤í…ì²˜ (SECOND, PV-RCNN) ì ìš©
- Transformer ê¸°ë°˜ 3D detector ì‹¤í—˜

## ğŸ“š ì°¸ê³ ë¬¸í—Œ

### ì£¼ìš” ë…¼ë¬¸
- **GCIoU Loss**: Ming et al. (2023). "Deep dive into gradients: Better optimization for 3D object detection with gradient-corrected IoU supervision." CVPR.
- **SGHMC**: Chen et al. (2014). "Stochastic gradient Hamiltonian Monte Carlo." ICML.
- **SGLD**: Welling & Teh (2011). "Bayesian learning via stochastic gradient Langevin dynamics." ICML.
- **SGNHT**: Ma et al. (2015). "A complete recipe for stochastic gradient MCMC." NeurIPS.

### ë°ì´í„°ì…‹ ë° í”„ë ˆì„ì›Œí¬
- **KITTI**: Geiger et al. (2012). "Are we ready for autonomous driving? The KITTI vision benchmark suite." CVPR.
- **PointPillars**: Lang et al. (2019). "PointPillars: Fast encoders for object detection from point clouds." CVPR.
- **OpenPCDet**: [GitHub Repository](https://github.com/open-mmlab/OpenPCDet)

## ğŸ“§ ì—°ë½ì²˜

- **ì €ì**: ê¹€ë‹¤ì—°
- **ì†Œì†**: [Your Institution]
- **ì´ë©”ì¼**: dayun0405@gmail.com

## ğŸ“„ ë¼ì´ì„ ìŠ¤

ì´ í”„ë¡œì íŠ¸ëŠ” í•™ìˆ  ì—°êµ¬ ëª©ì ìœ¼ë¡œ ê³µê°œë˜ì—ˆìŠµë‹ˆë‹¤. ìƒì—…ì  ì‚¬ìš© ì‹œ ì €ìì—ê²Œ ë¬¸ì˜í•´ì£¼ì‹œê¸° ë°”ëë‹ˆë‹¤.

## ğŸ™ ê°ì‚¬ì˜ ë§

ë³¸ ì—°êµ¬ëŠ” ë‹¤ìŒì˜ ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸ë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìˆ˜í–‰ë˜ì—ˆìŠµë‹ˆë‹¤:
- OpenPCDet
- KITTI Dataset
- PyTorch

---

**Note**: ë³¸ ì—°êµ¬ëŠ” íƒìƒ‰ì  ì—°êµ¬ë¡œì„œ, SGMCMC ê¸°ë²•ì´ ë³µì¡í•œ 3D ê°ì²´ ê²€ì¶œ ë¬¸ì œì—ì„œ í˜„ì¬ ì‹¤ë¬´ì  í•œê³„ë¥¼ ê°€ì§ì„ ë³´ì—¬ì¤ë‹ˆë‹¤. Uncertainty quantificationì´ í•„ìˆ˜ì ì¸ íŠ¹ìˆ˜í•œ ê²½ìš°ê°€ ì•„ë‹ˆë¼ë©´, Adamê³¼ ê°™ì€ adaptive optimizerë¥¼ ì‚¬ìš©í•˜ëŠ” ê²ƒì„ ê¶Œì¥í•©ë‹ˆë‹¤.
